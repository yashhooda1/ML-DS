{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f5f8af",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "- x = np.array([[200.0, 17.0]]) 1x2 matrix\n",
    "\n",
    "\n",
    "- layer_1 = Dense(units=3, activation='sigmoid') 3 neurons\n",
    "- a1 = layer_1(x)\n",
    "  - tf.Tensor([[0.2 0.7 0.3]]), shape(1,3), dtype=float32)  Tensor is a data type to representing matrix\n",
    "  - a1.numpy() to reverse it\n",
    "  \n",
    "\n",
    "- layer_2 = Dense(units=1, activation='sigmoid') 1 neuron\n",
    "- a2 = layer_2(a1)\n",
    "\n",
    "- w1_1 = np.array([1,2]) \n",
    "- b1_1 = np.array([-1])\n",
    "- z1_1 = np.dot(w1_1, x) + b\n",
    "- a1_1 = sigmoid(z1_1)\n",
    "\n",
    "\n",
    "- w1_2 = np.array([1,2])\n",
    "- b1_2 = np.array([-1])\n",
    "- z1_2 = np.dot(w1_2, x) + b\n",
    "- a1_2 = sigmoid(z1_2)\n",
    "\n",
    "\n",
    "- w1_3 = np.array([1,2])\n",
    "- b1_3 = np.array([-1])\n",
    "- z1_3 = np.dot(w1_3, x) + b\n",
    "- a1_3 = sigmoid(z1_3)\n",
    "\n",
    "\n",
    "- a1 = np.array([a1_1, a1_2, a1_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e285a6",
   "metadata": {},
   "source": [
    "## Model for digit classification\n",
    "- x = np.array([[0.0, ...245, ... 240...0]])\n",
    "\n",
    "\n",
    "- layer_1 = Dense(units=25, activation='sigmoid')\n",
    "- a1 = layer_1(x)\n",
    "\n",
    "\n",
    "- layer_2 = Dense(units=15, activation='sigmoid')\n",
    "- a2 = layer_2(a1)\n",
    "\n",
    "\n",
    "- layer_3 = Dense(units=1, activation='sigmoid')\n",
    "- a3 = layer_3(a2)\n",
    "\n",
    "- model=Sequential([layer_1, layer_2])\n",
    "\n",
    "- x = np.array([[...],\n",
    "                [...].\n",
    "                [...],\n",
    "                [...])\n",
    "\n",
    "- y = np.array([1. 0. 0, 1])\n",
    "- model.compile(...)\n",
    "- model.fit(x,y)\n",
    "- model.predict(x_new)\n",
    "\n",
    "- alternate approach\n",
    "\n",
    "- model = Sequential(\n",
    "                [Dense(units=25, activation='sigmoid'),\n",
    "                Dense(units=25, activation='sigmoid'),\n",
    "                Dense(units=25, activation='sigmoid')])\n",
    "- model.compile(...)\n",
    "- model.fit(x,y)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879dd5ed",
   "metadata": {},
   "source": [
    "## Forward Prop in NumPy\n",
    "W = np.array([1, -3, 5], <br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[2, 4, -6]])   2X3 <br>\n",
    "b = np.array([-1, 1, 2])  <br>\n",
    "a_in = np.array([-2, 4])  <br>\n",
    "\n",
    "def dense(a_in, W, b, g): <br>\n",
    "&emsp; &emsp;units = W.shape[1] <br>\n",
    "&emsp; &emsp;a_out = np.zeros(units) <br>\n",
    "&emsp; &emsp;for j in range(units): 0,1,2 <br>\n",
    "&emsp; &emsp;&emsp;w = W[:,j] <br>\n",
    "&emsp; &emsp;&emsp;z = np.dot(w, a_in) + b[j] <br>\n",
    "&emsp; &emsp;&emsp;a_out[j] = g(z) <br>\n",
    "&emsp; &emsp;return a_out\n",
    "   \n",
    "def sequential(x): <br>\n",
    "   &emsp; &emsp; a1 = dense(x, W1, b1) <br>\n",
    "   &emsp; &emsp; a2 = dense(a1, W2, b2) <br>\n",
    "   &emsp; &emsp; a3 = dense(a2, W3, b3) <br>\n",
    "   &emsp; &emsp; a4 = dense(a3, W4, b4) <br>\n",
    "   &emsp; &emsp; f_x = a4 <br>\n",
    "   &emsp; &emsp; return f_x\n",
    "   \n",
    "## Alternate approach\n",
    "def dense(A_in, W, B):<br>\n",
    "&emsp;Z = np.matmul(A_in, W) + B <br>\n",
    "&emsp;A_out = g(Z) <br>\n",
    "return A_out <br>\n",
    "\n",
    "a = [1  <br>\n",
    "&emsp;&emsp;2] <br>\n",
    "a_T = [1 2] <br>\n",
    "W = [3 5    <br>\n",
    "&emsp;&emsp; 4 6] <br>\n",
    "\n",
    "Z = a_T.W <br>\n",
    "Z = [11 17]\n",
    "\n",
    "Matrix Multiplicatio:  3 x 2 with 2 x 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ade19",
   "metadata": {},
   "source": [
    "## Training \n",
    "<img align=\"left\" src=\"Training Steps - Andrew NG.png\"     style=\" width:1000px; padding: 10px; \" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39ae9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # or PyTorch\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc821e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(units=25, activation='sigmoid'),\n",
    "        Dense(units=15, activation='sigmoid'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc22638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError\n",
    "model.compile(loss=BinaryCrossentropy()) # for Binary Classification problem\n",
    "#model.fit(X, y, epochs=100) # epochs: number of steps in gradient descent using \"back propagation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82a9217",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MeanSquaredError()) # for Regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f18c86",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "Sigmoid  0 < g(z) < 1 when y in <0, 1> <br>\n",
    "ReLU (Rectified Linear Unit): g(z) = max(0, z); if z<0 then g(z) is 0 else g(z) is Z when y <non negative values> <br>\n",
    "Linear Activiation Function: g(z) = z when y in <-1.2, 1.5, -0.4> don't use in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f15066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(units=25, activation='relu'),\n",
    "        Dense(units=15, activation='relu'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44101ed7",
   "metadata": {},
   "source": [
    "## Multi Class \n",
    "\n",
    "- Softmax regression (4 possible outputs) y = 1, 2, 3, 4 <br>\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "&emsp; z2 = W2 . X + b2 <br>\n",
    "&emsp; a2 = e^z2 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=2|x) <br>\n",
    "\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "&emsp; z1 = W1 . X + b1 <br>\n",
    "&emsp; a1 = e^z1 / (e^z1 + e^z2 + e^3z3 + e^4z4) <br>\n",
    "&emsp;&emsp;&ensp;= P(y=1|x) <br>\n",
    "\n",
    "Softmax Regression (N posssible outputs) <br>\n",
    "z_j = W_j . X + b_j  j=1,2,...,N <br>\n",
    "a_j = e^z_j / Sum(e^z_k) = P(y=j|X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ee5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
